
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
<!--     <meta name="viewport" content="width=device-width, initial-scale=1"> -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="Yizhe Zhu" content="">
    <link rel="icon" href="../../favicon.ico">

    <title>Yizhe Zhu's Homepage</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet">

    <!-- Custom styles for this template -->
   <!--  <link href="jumbotron.css" rel="stylesheet"> -->
    <link href="narrow-jumbotron.css" rel="stylesheet">

    <script src="https://use.fontawesome.com/65cdeb203c.js"></script>
   <!--  <script type="text/javascript" src="http://zhanghang1989.github.io/files/hidebib.js"></script> -->
    <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
  </head>

  <body>
    <div class="container">
      <div class="header clearfix">
        <nav>
          <ul class="nav nav-pills float-right">
            <li class="nav-item">
              <a class="nav-link active" href="#">Home <span class="sr-only">(current)</span></a>
            </li>
            <!-- <li class="nav-item">
              <a class="nav-link" href="#">About</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="#">Contact</a>
            </li> -->
          </ul>
        </nav>
        <!-- <h3 class="text-muted">Project name</h3> -->
    </div>

   

     
    <div class="container">
      <div class="col-xs-11">
    
      <div class="media-left">  <img src="figure/zhu.jpg" alt="..." width=" 220" height="220" class="media-object img-rounded"> </div>
      <div class="media-body">
      <h2 class="media-heading">Yizhe(Ethan) Zhu</h2>
      <h3 class="media-heading"><span lang="zh-cn">朱亦哲</span></h2>
     
      <p class="lead">I am a Ph.D. student in <a href="http://www.rutgers.edu/">Rutgers University</a>, supervised by <a href="https://www.cs.rutgers.edu/~elgammal/Home.html"> Professor Ahmed Elgammal</a>. My research interests include computer vision, machine learning and natural language processing.</p>
<!-- 
      <div class="col-xs-5 well col-sm-5"> -->
       <div class="col-sm-5" style="background-color: #F0F8FF;">
        <div class="row">
        <div class="col-lg-10">
        <h3>
        <a href="https://github.com/EthanZhu90" class="fa fa-github" aria-hidden="true"></a>
        <a href="https://www.linkedin.com/in/yizhe-ethan-zhu-171a06126/" class="fa fa-linkedin" aria-hidden="true"></a>
        <a href="https://www.facebook.com/yizhe.zhu" class="fa fa-facebook" aria-hidden="true"></a>
        <a href="mailto:yizhe.zhu@rutgers.edu" class="fa fa-envelope" aria-hidden="true"></a>
        <a href="https://scholar.google.com/citations?hl=en&user=hPXUR0cAAAAJ" class="fa fa-graduation-cap" aria-hidden="true"></a>
        <a href="figure/zhu_resume.pdf">[CV]</a>
        </h3>
        </div>
        </div>
      </div>

   
      </div>
      </div>
    </div>
    <hr>
       
        <!-- News -->
     <div class="container"> 
      <h2 class="media-heading">News</h2>
      </div>
    <div class="news" style="overflow:auto; height:150px; padding-top: 20px; padding-bottom: 5px;">
 
    <ul>
        <li>[Sep, 2019]  One paper is accepted to <b>NeurIPS2019</b>.
        <li>[July, 2019] Two papers are accepted to <b>ICCV2019</b>.
        <li>[Jun, 2019] I start an internship in NECLAB, Princeton.
        <li>[Oct, 2018] I have added the results of AUC in GZSL setting of our work <a href="https://github.com/EthanZhu90/ZSL_GAN_CVPR18">GAZSL</a>.
        </li>
        <li>[July, 2018] I have added the results on <a href="https://arxiv.org/abs/1707.00600">GBU setting</a>  of our work <a href="https://github.com/EthanZhu90/ZSL_GAN_CVPR18">GAZSL</a>.
        </li>
        <li>[Jun, 2018] I will attend <b>CVPR2018</b> in Salt Lake City.
        </li>
        <li>[Jun, 2018] I start to intern in Hikvision Research, CA.
        </li>
        <li>[Mar, 2018] I have released the source code and dataset of <a href="https://github.com/EthanZhu90/ZSL_GAN_CVPR18"> CVPR18 paper GAZSL</a>.
        </li>
        <li>[Mar, 2018] My son Michael was born. It's the <b>BEST</b> gift to me.
        </li>
        <li>[Feb, 2017] One paper is accepted to <b>CVPR2018</b>.
        
    </ul>
    </div>

    <div class="container" style="padding-top: 50px;">
    </div>
    
    <div class="container"> 
      <h2 class="media-heading">Publications</h2>
      <h4 class="media-heading">(* indicates equal contributions)</h4>
 <!-- paper NeurIPS19 -->
    <div class="media-left">  <img src="figure/NeurIPS19.png" alt="..." width=" 250" height="110" class="media-object img-rounded"> </div>
    <div class="media-right">
      <p><font size ="3.5">
        <heading><b>[NeurIPS'19]</b> Semantic-Guided Multi-Attention Localization for Zero-Shot Learning
        </heading><br>
        <u><b>Yizhe Zhu</b></u>, 
        <a href="http://www.stat.ucla.edu/~jxie/">Jianwen Xie</a>, 
        <a href="https://sites.google.com/site/zhiqiangtanghomepage/home">Zhiqiang Tang</a>, 
        <a href="https://sites.google.com/site/xipengcshomepage/home">Xi Peng </a>, 
        <a href="https://www.cs.rutgers.edu/~elgammal/Home.html">Ahmed Elgammal</a><br>
        <em>Thirty-third Conference on Neural Information Processing Systems</em>,  Vancouver, Canada, Dec. 2019<br>
        <a href="https://arxiv.org/abs/1903.00502">paper</a> |
        <a onclick="togglediv('NeurIPS19_MA')">abstract</a> |
        <a onclick="togglediv('NeurIPS19_MA_bibtex')">bibtex</a> |
        <a>poster</a> |
        <a>code</a> <br>
        <!--<p xml:space="preserve"> <i id="CVPR18_GAZSL" style="display: none;"> -->
        <div id = "NeurIPS19_MA", style="display: none">
         Zero-shot learning extends the conventional object classification to the unseen class
recognition by introducing semantic representations of classes. Existing approaches
predominantly focus on learning the proper mapping function for visual-semantic
embedding, while neglecting the effect of learning discriminative visual features. In
this paper, we study the significance of the discriminative region localization. We
propose a semantic-guided multi-attention localization model, which automatically
discovers the most discriminative parts of objects for zero-shot learning without any
human annotations. Our model jointly learns cooperative global and local features
from the whole object as well as the detected parts to categorize objects based on
semantic descriptions. Moreover, with the joint supervision of embedding softmax
loss and class-center triplet loss, the model is encouraged to learn features with
high inter-class dispersion and intra-class compactness. Through comprehensive
experiments on three widely used zero-shot learning benchmarks, we show the
efficacy of the multi-attention localization and our proposed approach improves the
state-of-the-art results by a considerable margin.
        </div>
        
<div id = "NeurIPS19_MA_bibtex", style="display: none">
@article{Yizhe_2019_ICCV,
title={Semantic-Guided Multi-Attention Localization for Zero-Shot Learning},
author={Zhu, Yizhe and Xie, Jianwen and Tang, Zhiqiang and Peng, Xi and Elgammal, Ahmed},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2019}
}
  
</div>
        
          <!-- </i></p> -->
        <!-- <a href="http://eceweb1.rutgers.edu/vision/gts/gtos.html">project</a>  -->
        </font>
      </p>
    </div>
    <hr>      
      
<!-- paper ICCV19 -->
    <div class="media-left">  <img src="figure/ABP_ICCV19.png" alt="..." width=" 250" height="110" class="media-object img-rounded"> </div>
    <div class="media-right">
      <p><font size ="3.5">
        <heading><b>[ICCV'19]</b> Learning Feature-to-Feature Translator by Alternating Back-Propagation for Zero-Shot Learning
        </heading><br>
        <u><b>Yizhe Zhu</b></u>, 
        <a href="http://www.stat.ucla.edu/~jxie/">  Jianwen Xie </a>, 
        Bingchen Liu, 
        <a href="https://www.cs.rutgers.edu/~elgammal/Home.html"> Ahmed Elgammal</a><br>
        <em>International Conference on Computer Vision</em>, Seoul, Oct. 2019<br>
        <a href="https://arxiv.org/abs/1904.10056">paper</a> |
        <a onclick="togglediv('ICCV19_ABP')">abstract</a> |
        <a>poster</a> |
        <a>code</a> <br>
        <!--<p xml:space="preserve"> <i id="CVPR18_GAZSL" style="display: none;"> -->
        <div id = "ICCV19_ABPZSL", style="display: none">
         We investigate learning feature-to-feature translator
networks by alternating back-propagation as a generalpurpose solution to zero-shot learning (ZSL) problems. Our
method can be categorized to a generative model-based ZSL
one. In contrast to the GAN or VAE that requires auxiliary networks to assist the training, our model consists of
a single conditional generator that maps the class feature
and a latent vector to the image feature, and is trained by
maximum likelihood estimation. The training process is a
simple yet effective EM-like process that iterates the following two steps: (i) the inferential back-propagation to infer
the latent noise vector of each observed data, and (ii) the
learning back-propagation to update the parameters of the
model. With slight modifications of our model, we also provide a solution to learning from incomplete visual features
for ZSL. We conduct extensive comparisons with existing
generative ZSL methods on five benchmarks, demonstrating the superiority of our method in not only performance
but also convergence speed and computational cost. Specifically, our model outperforms the existing state-of-the-art
methods by a remarkable margin up to 3.1% and 4.0% in
ZSL and generalized ZSL settings, respectively.
        </div>
          <!-- </i></p> -->
        <!-- <a href="http://eceweb1.rutgers.edu/vision/gts/gtos.html">project</a>  -->
        </font>
      </p>
    </div>
    <hr>
      
       <!-- paper CVPR18 -->
    <div class="media-left">  <img src="figure/cvpr18.PNG" alt="..." width=" 250" height="110" class="media-object img-rounded"> </div>
    <div class="media-right">
      <p><font size ="3.5">
        <heading><b>[CVPR'18]</b> A Generative Adversarial  Approach
          for Zero-Shot Learning from Noisy Texts
        </heading><br>
        <u><b>Yizhe Zhu</b></u>, 
        <a href="https://sites.google.com/site/mhelhoseiny/"> Mohamed Elhoseiny</a>, 
        Bingchen Liu, 
        <a href="https://sites.google.com/site/xipengcshomepage/home?authuser=0"> Xi Peng </a>,
        <a href="https://www.cs.rutgers.edu/~elgammal/Home.html"> Ahmed Elgammal</a><br>
        <em>IEEE Conference on Computer Vision and Pattern Recognition</em>, Salt Lake City, June 2018<br>
        <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhu_A_Generative_Adversarial_CVPR_2018_paper.pdf">paper</a> |
        <a onclick="togglediv('CVPR18_GAZSL')">abstract</a> |
        <a href="figure/GAN_ZSL_CVPR18.pdf">poster</a> |
        <a href="https://github.com/EthanZhu90/ZSL_GAN_CVPR18">code</a> <br>
        <!--<p xml:space="preserve"> <i id="CVPR18_GAZSL" style="display: none;"> -->
        <div id = "CVPR18_GAZSL", style="display: none">
         Most existing zero-shot learning methods consider the problem as a visual semantic embedding one. Given the demonstrated capability of Generative Adversarial Networks(GANs) to generate images, we instead leverage GANs to imagine unseen categories from text descriptions and hence recognize novel classes with no examples being seen. Specifically, we propose a simple yet effective generative model that takes as input noisy text descriptions about an unseen class (e.g.Wikipedia articles) and generates synthesized visual features for this class. With added pseudo data, zero-shot learning is naturally converted to a traditional classification problem. Additionally, to preserve the inter-class discrimination of the generated features, a visual pivot regularization is proposed as an explicit supervision.
Unlike previous methods using complex engineered regularizers, our approach can suppress the noise well without additional regularization. Empirically, we show that our method consistently outperforms the state of the art on the largest available benchmarks on Text-based Zero-shot Learning.  
        </div>
          <!-- </i></p> -->
        <!-- <a href="http://eceweb1.rutgers.edu/vision/gts/gtos.html">project</a>  -->
        </font>
      </p>
    </div>
    <hr>
    
      <!-- paper ICCV17 -->
      <div class="media-left">  <img src="figure/iccv17.png" alt="..." width=" 250" height="130" class="media-object img-rounded"> </div>
      <div class="media-right">
        <p> <font size ="3.5">
          <heading><b>[ICCV'17]</b> A Multilayer-Based Framework for Online Background Subtraction with Freely Moving Cameras</heading><br>
          <u><b>Yizhe Zhu</b></u>, 
          <a href="https://www.cs.rutgers.edu/~elgammal/Home.html"> Ahmed Elgammal</a><br>
          <em>International Conference on Computer Vision</em>, Venice, Oct. 2017<br>
          <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_A_Multilayer-Based_Framework_ICCV_2017_paper.pdf">paper</a> | 
          <a href="http://openaccess.thecvf.com/content_ICCV_2017/supplemental/Zhu_A_Multilayer-Based_Framework_ICCV_2017_supplemental.pdf">supp</a> |
      
          <a onclick="togglediv('ICCV17_abs')">abstract</a> |
          <a href="figure/MultilayerBSMC_ICCV17.pdf">poster</a> |
          <a href="https://github.com/EthanZhu90/MultilayerBSMC">code</a> <br>

          <div id = "ICCV17_abs", style="display: none">
          The exponentially increasing use of moving platforms for video capture introduces the urgent need to develop the general background subtraction algorithms with the capability to deal with the moving background. In this paper, we propose a multilayer-based framework for online background subtraction for videos captured by moving cameras. Unlike the previous treatments of the problem, the proposed method is not restricted to binary segmentation of background and foreground, but formulates it as a multi-label segmentation problem by modeling multiple foreground objects in different layers when they appear simultaneously in the scene. We assign an independent processing layer to each foreground object, as well as the background, where both motion and appearance models are estimated, and a probability map is inferred using a Bayesian filtering framework. Finally, Multi-label Graph-cut on Markov Random Field is employed to perform pixel-wise labeling. Extensive evaluation results show that the proposed method outperforms state-of-the-art methods on challenging video sequences.
          </div>
          </font>
          </p>
      </div>
      <hr>

 
    <!-- paper CVPR17 -->
    <div class="media-left">  <img src="figure/cvpr17.png" alt="..." width=" 250" height="130" class="media-object img-rounded"> </div>
    <div class="media-right">
      <p><font size ="3.5">
        <heading><b>[CVPR'17]</b> Link the head to the “beak”: Zero Shot Learning
        from Noisy Text Description at Part Precision</heading><br>
        <a href="https://sites.google.com/site/mhelhoseiny/"> Mohamed Elhoseiny</a>*, 
        <u><b>Yizhe Zhu*</b></u>, 
        <a href="http://paul.rutgers.edu/~hz138/"> Han Zhang</a>, 
        <a href="https://www.cs.rutgers.edu/~elgammal/Home.html"> Ahmed Elgammal</a><br>
        <em>IEEE Conference on Computer Vision and Pattern Recognition</em>, Hawaii, July 2017<br>
        <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Elhoseiny_Link_the_Head_CVPR_2017_paper.pdf">paper</a> |
        <a href="http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Elhoseiny_Link_the_Head_2017_CVPR_supplemental.pdf">supp</a> |
        <a onclick="togglediv('CVPR17_abs')">abstract</a> |
        <a href="https://github.com/EthanZhu90/ZSL_PP">code</a> <br>
        <div id = "CVPR17_abs", style="display: none">
         In this paper, we study learning visual classifiers from unstructured text descriptions at part precision with no training images. We propose a learning framework that is able to connect text terms to its relevant parts and suppress connections to non-visual text terms without any part-text annotations.  For instance, this learning process enables terms like ``beak'' to be sparsely linked to the visual representation of parts like head, while reduces the effect of non-visual terms like ``migrate'' on classifier prediction.  Images are encoded by a part-based CNN that detect bird parts and learn part-specific representation. Part-based visual classifiers are predicted from text descriptions of unseen visual classifiers to facilitate classification without training images (also known as zero-shot recognition). We performed our experiments on CUBirds 2011 dataset and improves the state-of-the-art text-based zero-shot recognition results from 34.7\% to 43.6\%. We also created large scale benchmarks on North American Bird Images augmented with text descriptions, where we also show that our approach outperforms existing methods. 
         </div>
        </font>
      </p>
    </div>
    <hr>  
    
    <script>
    function togglediv(id) {
        var div = document.getElementById(id);
        div.style.display = div.style.display == "none" ? "inline" : "none";
    }
    </script>
     <!-- block -->
    <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=uUnZk8Z2r5iKnbxl6FtRkMDbwLa7jor51ZHMC5PgM9s"></script>
    
   
      <footer class="footer">
        <p>&copy; 2017. All rights reserved.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="../../../../assets/js/ie10-viewport-bug-workaround.js"></script>
  </body>
</html>
