
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
<!--     <meta name="viewport" content="width=device-width, initial-scale=1"> -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="Yizhe Zhu" content="">
    <link rel="icon" href="../../favicon.ico">

    <title>Yizhe Zhu's Homepage</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet">

    <!-- Custom styles for this template -->
   <!--  <link href="jumbotron.css" rel="stylesheet"> -->
    <link href="narrow-jumbotron.css" rel="stylesheet">

    <script src="https://use.fontawesome.com/65cdeb203c.js"></script>
   <!--  <script type="text/javascript" src="http://zhanghang1989.github.io/files/hidebib.js"></script> -->
    <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
  </head>

  <body>
    <div class="container">
      <div class="header clearfix">
        <nav>
          <ul class="nav nav-pills float-right">
            <li class="nav-item"><a class="nav-link active" href="/">Home </a></li>
            <li class="nav-item"><a class="" href="./publications/">Publications</a></li>
            <!-- <li class="nav-item">
              <a class="nav-link" href="#">About</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="#">Contact</a>
            </li> -->
          </ul>
        </nav>
        <!-- <h3 class="text-muted">Project name</h3> -->
    </div>

     
    <div class="container">
      <div class="col-xs-11">
    
      <div class="media-left">  <img src="figure/zhu.jpg" alt="..." width=" 220" height="220" class="media-object img-rounded"> </div>
      <div class="media-body">
      <h2 class="media-heading">Yizhe(Ethan) Zhu</h2>
      <h3 class="media-heading"><span lang="zh-cn">朱亦哲</span></h2>
     
      <p class="lead">I am a Ph.D. student in <a href="http://www.rutgers.edu/">Rutgers University</a>, supervised by <a href="https://www.cs.rutgers.edu/~elgammal/Home.html"> Professor Ahmed Elgammal</a>.
        My research interests include computer vision, machine learning and natural language processing. I'm on the job market.</p>
<!-- 
      <div class="col-xs-5 well col-sm-5"> -->
       <div class="col-sm-5" style="background-color: #F0F8FF;">
        <div class="row">
        <div class="col-lg-10">
        <h3>
        <a href="https://github.com/EthanZhu90" class="fa fa-github" aria-hidden="true"></a>
        <a href="https://www.linkedin.com/in/yizhe-ethan-zhu-171a06126/" class="fa fa-linkedin" aria-hidden="true"></a>
        <a href="https://www.facebook.com/yizhe.zhu" class="fa fa-facebook" aria-hidden="true"></a>
        <a href="mailto:yizhe.zhu@rutgers.edu" class="fa fa-envelope" aria-hidden="true"></a>
        <a href="https://scholar.google.com/citations?hl=en&user=hPXUR0cAAAAJ" class="fa fa-graduation-cap" aria-hidden="true"></a>
        <a href="figure/zhu_resume.pdf">[CV]</a>
        </h3>
        </div>
        </div>
      </div>

   
      </div>
      </div>
    </div>
    <hr>
       
        <!-- News -->
     <div class="container"> 
      <h2 class="media-heading">News</h2>
      </div>
    <div class="news" style="overflow:auto; height:150px; padding-top: 20px; padding-bottom: 5px;">
 
    <ul>
        <li>[Feb, 2020]  One paper is accepted to <b>CVPR 2020</b>.
        <li>[Dec, 2019]  One paper is accepted to <b>ICLR 2020</b>.
        <li>[Nov, 2019]  One paper is accepted to <b>AAAI 2020</b>.
        <li>[Oct, 2019]  I have released the source code of <a href="https://github.com/EthanZhu90/ZSL_ABP"> ICCV2019 paper ZSL_ABP</a>.
        <li>[Sep, 2019]  One paper is accepted to <b>NeurIPS2019</b>.
        <li>[July, 2019] Two papers are accepted to <b>ICCV2019</b>.
        <li>[Jun, 2019] I start an internship in NECLAB, Princeton.
        <li>[Oct, 2018] I have added the results of AUC in GZSL setting of our work <a href="https://github.com/EthanZhu90/ZSL_GAN_CVPR18">GAZSL</a>.
        </li>
        <li>[July, 2018] I have added the results on <a href="https://arxiv.org/abs/1707.00600">GBU setting</a>  of our work <a href="https://github.com/EthanZhu90/ZSL_GAN_CVPR18">GAZSL</a>.
        </li>
        <li>[Jun, 2018] I will attend <b>CVPR2018</b> in Salt Lake City.
        </li>
        <li>[Jun, 2018] I start to intern in Hikvision Research, CA.
        </li>
        <li>[Mar, 2018] I have released the source code and dataset of <a href="https://github.com/EthanZhu90/ZSL_GAN_CVPR18"> CVPR18 paper GAZSL</a>.
        </li>
        <li>[Mar, 2018] My son Michael was born. It's the <b>BEST</b> gift to me.
        </li>
        <li>[Feb, 2017] One paper is accepted to <b>CVPR2018</b>.
        
    </ul>
    </div>

    <div class="container" style="padding-top: 50px;">
    </div>
    
    <div class="container"> 
      <h2 class="media-heading">Selected Publications</h2>
      <h4 class="media-heading">(* indicates equal contributions)</h4>
        <!-- paper CVPR20 -->
    <div class="media-left">  <img src="figure/CVPR2020.PNG" alt="..." width=" 250" height="110" class="media-object img-rounded"> </div>
    <div class="media-right">
      <p><font size ="3.5">
        <heading><b>[CVPR'20]</b> S3VAE: Self-Supervised Sequential VAE for Representation Disentanglement and Data Generation
        </heading><br>
        <u><b>Yizhe Zhu</b></u>, 
        <a href="http://www.cs.toronto.edu/~cuty/">Martin Renqiang Min</a>, 
        <a href="http://www.nec-labs.com/asim-kadav">Asim Kadav</a>, 
        <a href="http://www.nec-labs.com/hans-peter-graf">Hans Peter Graf</a> <br>
        <em>IEEE Conference on Computer Vision and Pattern Recognition</em>, Seattle, June 2020<br>
        <a href="figure/S3VAE.pdf">paper</a> |
        <a onclick="togglediv('CVPR20_S3VAE')">abstract</a> |
        <a onclick="togglediv('CVPR20_S3VAE_bibtex')">bibtex</a>--> |
        <a href="figure/S3VAE-slides.pdf">slides</a>  |
        code <br>
        <!--<p xml:space="preserve"> <i id="CVPR18_GAZSL" style="display: none;"> -->
        <div id = "CVPR20_S3VAE", style="display: none">
          We propose a sequential variational autoencoder to learn disentangled representations of sequential data (e.g., videos and audios) under self-supervision. Specifically, we exploit the benefits of some readily accessible supervision signals from input data itself or some off-the-shelf functional models and accordingly design auxiliary tasks for our model to utilize these signals. With the supervision of the signals, our model can easily disentangle the representation of an input sequence into static factors and dynamic factors (i.e., time-invariant and time-varying parts). Comprehensive experiments across videos and audios verify the effectiveness of our model on representation disentanglement and generation of sequential data, and demonstrate that, our model with self-supervision performs comparable to, if not better than, the fully-supervised model with ground truth labels, and outperforms state-of-the-art unsupervised models by a large margin. As far as we know, our model is the first that employs self-supervision for disentangled representation learning of sequential data.
        </div>

        <div id = "CVPR20_S3VAE_bibtex", style="display: none">
        @inproceedings{zhu2020S3VAE,<br>
          title={S3VAE: Self-Supervised Sequential VAE for Representation Disentanglement and  Data Generation},<br>
          author={Zhu, Yizhe and Min, Martin Renqiang and Kadav, Asim and Graf, Hans Peter},<br>
          booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br>
          year={2020} <br>
          }
          </div>
        </div>
        
          <!-- </i></p> -->
        <!-- <a href="http://eceweb1.rutgers.edu/vision/gts/gtos.html">project</a>  -->
        </font>
      </p>
    </div>
    <hr>      
      
      
      
      
     <!-- paper NeurIPS19 -->
    <div class="media-left">  <img src="figure/NeurIPS19.png" alt="..." width=" 250" height="110" class="media-object img-rounded"> </div>
    <div class="media-right">
      <p><font size ="3.5">
        <heading><b>[NeurIPS'19]</b> Semantic-Guided Multi-Attention Localization for Zero-Shot Learning
        </heading><br>
        <u><b>Yizhe Zhu</b></u>, 
        <a href="http://www.stat.ucla.edu/~jxie/">Jianwen Xie</a>, 
        <a href="https://sites.google.com/site/zhiqiangtanghomepage/home">Zhiqiang Tang</a>, 
        <a href="https://sites.google.com/site/xipengcshomepage/home">Xi Peng </a>, 
        <a href="https://www.cs.rutgers.edu/~elgammal/Home.html">Ahmed Elgammal</a><br>
        <em>Thirty-third Conference on Neural Information Processing Systems</em>,  Vancouver, Canada, Dec. 2019<br>
        <a href="https://arxiv.org/abs/1903.00502">paper</a> |
        <a onclick="togglediv('NeurIPS19_MA')">abstract</a> |
        <a onclick="togglediv('NeurIPS19_MA_bibtex')">bibtex</a> |
        <a href="figure/MAZSL_NIPS2019.pdf">poster</a> |
        code <br>
        <!--<p xml:space="preserve"> <i id="CVPR18_GAZSL" style="display: none;"> -->
        <div id = "NeurIPS19_MA", style="display: none">
          Zero-shot learning extends the conventional object classification to the unseen class
          recognition by introducing semantic representations of classes. Existing approaches
          predominantly focus on learning the proper mapping function for visual-semantic
          embedding, while neglecting the effect of learning discriminative visual features. In
          this paper, we study the significance of the discriminative region localization. We
          propose a semantic-guided multi-attention localization model, which automatically
          discovers the most discriminative parts of objects for zero-shot learning without any
          human annotations. Our model jointly learns cooperative global and local features
          from the whole object as well as the detected parts to categorize objects based on
          semantic descriptions. Moreover, with the joint supervision of embedding softmax
          loss and class-center triplet loss, the model is encouraged to learn features with
          high inter-class dispersion and intra-class compactness. Through comprehensive
          experiments on three widely used zero-shot learning benchmarks, we show the
          efficacy of the multi-attention localization and our proposed approach improves the
          state-of-the-art results by a considerable margin.
        </div>

      <div id = "NeurIPS19_MA_bibtex", style="display: none">
      @InProceedings{zhu2019semantic, <br>
      title={Semantic-Guided Multi-Attention Localization for Zero-Shot Learning}, <br>
      author={Zhu, Yizhe and Xie, Jianwen and Tang, Zhiqiang and Peng, Xi and Elgammal, Ahmed},  <br>
      booktitle = {Thirty-third Conference on Neural Information Processing Systems (NeurIPS)},  <br>
      month = {Dec},  <br>
      year = {2019} <br>
      }
      </div>
        
          <!-- </i></p> -->
        <!-- <a href="http://eceweb1.rutgers.edu/vision/gts/gtos.html">project</a>  -->
        </font>
      </p>
    </div>
    <hr>      
      
<!-- paper ICCV19 -->
    <div class="media-left">  <img src="figure/ABP_ICCV19.png" alt="..." width=" 250" height="110" class="media-object img-rounded"> </div>
    <div class="media-right">
      <p><font size ="3.5">
        <heading><b>[ICCV'19]</b> Learning Feature-to-Feature Translator by Alternating Back-Propagation for Zero-Shot Learning
        </heading><br>
        <u><b>Yizhe Zhu</b></u>, 
        <a href="http://www.stat.ucla.edu/~jxie/">  Jianwen Xie </a>, 
        Bingchen Liu, 
        <a href="https://www.cs.rutgers.edu/~elgammal/Home.html"> Ahmed Elgammal</a><br>
        <em>International Conference on Computer Vision</em>, Seoul, Oct. 2019<br>
        <a href="https://arxiv.org/abs/1904.10056">paper</a> |
        <a onclick="togglediv('ICCV19_ABP')">abstract</a> |
        <a onclick="togglediv('ICCV19_bibtex')">bibtex</a> |
        <a href="figure/ZSL_ABP_ICCV19.pdf">poster</a> |
        <a href="https://github.com/EthanZhu90/ZSL_ABP">code</a> <br>
        <!--<p xml:space="preserve"> <i id="CVPR18_GAZSL" style="display: none;"> -->
        <div id = "ICCV19_ABP", style="display: none">
        We investigate learning feature-to-feature translator networks by alternating back-propagation as a general purpose solution to zero-shot learning (ZSL) problems. Our
        method can be categorized to a generative model-based ZSL one. In contrast to the GAN or VAE that requires auxiliary networks to assist the training, our model consists of
        a single conditional generator that maps the class feature and a latent vector to the image feature, and is trained by maximum likelihood estimation. The training process is a
        simple yet effective EM-like process that iterates the following two steps: (i) the inferential back-propagation to infer
        the latent noise vector of each observed data, and (ii) the learning back-propagation to update the parameters of the
        model. With slight modifications of our model, we also provide a solution to learning from incomplete visual features
        for ZSL. We conduct extensive comparisons with existing generative ZSL methods on five benchmarks, demonstrating the superiority of our method in not only performance
        but also convergence speed and computational cost. Specifically, our model outperforms the existing state-of-the-art
        methods by a remarkable margin up to 3.1% and 4.0% in ZSL and generalized ZSL settings, respectively.
        </div>
        <div id = "ICCV19_bibtex", style="display: none">
        @InProceedings{zhu2019learning, <br>
        title={Learning Feature-to-Feature Translator by Alternating Back-Propagation for Generative Zero-Shot Learning}, <br>
        author={Zhu, Yizhe and Xie, Jianwen and Liu, Bingchen and Elgammal, Ahmed},  <br>
        booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},  <br>
        month = {Oct},  <br>
        year = {2019} <br>
        }
        </div>
        
          <!-- </i></p> -->
        <!-- <a href="http://eceweb1.rutgers.edu/vision/gts/gtos.html">project</a>  -->
        </font>
      </p>
    </div>
    <hr>
      
       <!-- paper CVPR18 -->
    <div class="media-left">  <img src="figure/cvpr18.PNG" alt="..." width=" 250" height="110" class="media-object img-rounded"> </div>
    <div class="media-right">
      <p><font size ="3.5">
        <heading><b>[CVPR'18]</b> A Generative Adversarial  Approach for Zero-Shot Learning from Noisy Texts
        </heading><br>
        <u><b>Yizhe Zhu</b></u>, 
        <a href="https://sites.google.com/site/mhelhoseiny/"> Mohamed Elhoseiny</a>, 
        Bingchen Liu, 
        <a href="https://sites.google.com/site/xipengcshomepage/home?authuser=0"> Xi Peng </a>,
        <a href="https://www.cs.rutgers.edu/~elgammal/Home.html"> Ahmed Elgammal</a><br>
        <em>IEEE Conference on Computer Vision and Pattern Recognition</em>, Salt Lake City, June 2018<br>
        <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhu_A_Generative_Adversarial_CVPR_2018_paper.pdf">paper</a> |
        <a onclick="togglediv('CVPR18_GAZSL')">abstract</a> |
        <a onclick="togglediv('CVPR18_bibtex')">bibtex</a> |
        <a href="figure/GAN_ZSL_CVPR18.pdf">poster</a> |
        <a href="https://github.com/EthanZhu90/ZSL_GAN_CVPR18">code</a> <br>
        <!--<p xml:space="preserve"> <i id="CVPR18_GAZSL" style="display: none;"> -->
        <div id = "CVPR18_GAZSL", style="display: none">
         Most existing zero-shot learning methods consider the problem as a visual semantic embedding one. Given the demonstrated capability of Generative Adversarial Networks(GANs) to generate images, we instead leverage GANs to imagine unseen categories from text descriptions and hence recognize novel classes with no examples being seen. Specifically, we propose a simple yet effective generative model that takes as input noisy text descriptions about an unseen class (e.g.Wikipedia articles) and generates synthesized visual features for this class. With added pseudo data, zero-shot learning is naturally converted to a traditional classification problem. Additionally, to preserve the inter-class discrimination of the generated features, a visual pivot regularization is proposed as an explicit supervision.
Unlike previous methods using complex engineered regularizers, our approach can suppress the noise well without additional regularization. Empirically, we show that our method consistently outperforms the state of the art on the largest available benchmarks on Text-based Zero-shot Learning.  
        </div>
        
        <div id = "CVPR18_bibtex", style="display: none">        
        @inproceedings{zhu2018generative,<br>
        title={A generative adversarial approach for zero-shot learning from noisy texts},<br>
        author={Zhu, Yizhe and Elhoseiny, Mohamed and Liu, Bingchen and Peng, Xi and Elgammal, Ahmed},<br>
        booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br>
        pages={1004--1013},<br>
        year={2018} <br>
        }
        </div>
        
          <!-- </i></p> -->
        <!-- <a href="http://eceweb1.rutgers.edu/vision/gts/gtos.html">project</a>  -->
        </font>
      </p>
    </div>
    <hr>
    
      <!-- paper ICCV17 -->
      <div class="media-left">  <img src="figure/iccv17.png" alt="..." width=" 250" height="130" class="media-object img-rounded"> </div>
      <div class="media-right">
        <p> <font size ="3.5">
          <heading><b>[ICCV'17]</b> A Multilayer-Based Framework for Online Background Subtraction with Freely Moving Cameras</heading><br>
          <u><b>Yizhe Zhu</b></u>, 
          <a href="https://www.cs.rutgers.edu/~elgammal/Home.html"> Ahmed Elgammal</a><br>
          <em>International Conference on Computer Vision</em>, Venice, Oct. 2017<br>
          <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_A_Multilayer-Based_Framework_ICCV_2017_paper.pdf">paper</a> | 
          <a href="http://openaccess.thecvf.com/content_ICCV_2017/supplemental/Zhu_A_Multilayer-Based_Framework_ICCV_2017_supplemental.pdf">supp</a> |
      
          <a onclick="togglediv('ICCV17_abs')">abstract</a> |
          <a onclick="togglediv('ICCV17_bibtex')">bibtex</a> |
          <a href="figure/MultilayerBSMC_ICCV17.pdf">poster</a> |
          <a href="https://github.com/EthanZhu90/MultilayerBSMC">code</a> <br>

          <div id = "ICCV17_abs", style="display: none">
          The exponentially increasing use of moving platforms for video capture introduces the urgent need to develop the general background subtraction algorithms with the capability to deal with the moving background. In this paper, we propose a multilayer-based framework for online background subtraction for videos captured by moving cameras. Unlike the previous treatments of the problem, the proposed method is not restricted to binary segmentation of background and foreground, but formulates it as a multi-label segmentation problem by modeling multiple foreground objects in different layers when they appear simultaneously in the scene. We assign an independent processing layer to each foreground object, as well as the background, where both motion and appearance models are estimated, and a probability map is inferred using a Bayesian filtering framework. Finally, Multi-label Graph-cut on Markov Random Field is employed to perform pixel-wise labeling. Extensive evaluation results show that the proposed method outperforms state-of-the-art methods on challenging video sequences.
          </div>
          <div id = "ICCV17_bibtex", style="display: none">
           @inproceedings{zhu2017multilayer, <br>
            title={A multilayer-based framework for online background subtraction with freely moving cameras}, <br>
            author={Zhu, Yizhe and Elgammal, Ahmed}, <br>
            booktitle={Proceedings of the IEEE International Conference on Computer Vision (ICCV)}, <br>
            pages={5132--5141}, <br>
            year={2017} <br>
            }
          </div>
                
          </font>
          </p>
      </div>
      <hr>

 
    <!-- paper CVPR17 -->
    <div class="media-left">  <img src="figure/cvpr17.png" alt="..." width=" 250" height="130" class="media-object img-rounded"> </div>
    <div class="media-right">
      <p><font size ="3.5">
        <heading><b>[CVPR'17]</b> Link the head to the “beak”: Zero Shot Learning
        from Noisy Text Description at Part Precision</heading><br>
        <a href="https://sites.google.com/site/mhelhoseiny/"> Mohamed Elhoseiny</a>*, 
        <u><b>Yizhe Zhu*</b></u>, 
        <a href="http://paul.rutgers.edu/~hz138/"> Han Zhang</a>, 
        <a href="https://www.cs.rutgers.edu/~elgammal/Home.html"> Ahmed Elgammal</a><br>
        <em>IEEE Conference on Computer Vision and Pattern Recognition</em>, Hawaii, July 2017<br>
        <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Elhoseiny_Link_the_Head_CVPR_2017_paper.pdf">paper</a> |
        <a href="http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Elhoseiny_Link_the_Head_2017_CVPR_supplemental.pdf">supp</a> |
        <a onclick="togglediv('CVPR17_abs')">abstract</a> |
        <a onclick="togglediv('CVPR17_bibtex')">bibtex</a> |
        <a href="https://github.com/EthanZhu90/ZSL_PP">code</a> <br>
        <div id = "CVPR17_abs", style="display: none">
         In this paper, we study learning visual classifiers from unstructured text descriptions at part precision with no training images. We propose a learning framework that is able to connect text terms to its relevant parts and suppress connections to non-visual text terms without any part-text annotations.  For instance, this learning process enables terms like ``beak'' to be sparsely linked to the visual representation of parts like head, while reduces the effect of non-visual terms like ``migrate'' on classifier prediction.  Images are encoded by a part-based CNN that detect bird parts and learn part-specific representation. Part-based visual classifiers are predicted from text descriptions of unseen visual classifiers to facilitate classification without training images (also known as zero-shot recognition). We performed our experiments on CUBirds 2011 dataset and improves the state-of-the-art text-based zero-shot recognition results from 34.7\% to 43.6\%. We also created large scale benchmarks on North American Bird Images augmented with text descriptions, where we also show that our approach outperforms existing methods. 
         </div>
         <div id = "CVPR17_bibtex", style="display: none">
           @inproceedings{elhoseiny2017link, <br>
              title={Link the head to the ``beak": Zero shot learning from noisy text description at part precision}, <br>
              author={Elhoseiny, Mohamed and Zhu, Yizhe and Zhang, Han and Elgammal, Ahmed}, <br>
              booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, <br>
              pages={6288--6297}, <br>
              year={2017}, <br>
              organization={IEEE} <br>
            }
          </div>
        
        
        </font>
      </p>
    </div>
    <hr>  
    
    <script>
    function togglediv(id) {
        var div = document.getElementById(id);
        div.style.display = div.style.display == "none" ? "block" : "none";
    }
    </script>
      
      
      
     <!-- block -->
    <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=uUnZk8Z2r5iKnbxl6FtRkMDbwLa7jor51ZHMC5PgM9s"></script>
    
   
      <footer class="footer">
        <p>&copy; 2017. All rights reserved.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="../../../../assets/js/ie10-viewport-bug-workaround.js"></script>
  </body>
</html>
